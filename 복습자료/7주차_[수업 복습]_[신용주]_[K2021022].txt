앙상블 학습
- 여러개의 분류기를 생성하고 그 예측값을 결합함으로써 보다 정확한 최종예측을 도출하는 기법
- 단일분류기보다 신뢰성이 높은 예측값을 얻는 것 
- 비정형 데이터의 분류는 딥러닝이 뛰어난 성능을 보임
- 그러나 대부분의 정형데이터는 앙상블이 뛰어남 
- 대표적인 기법 랜덤포레스트 , XGboost 
- 앙상블은 보팅, 배깅, 부스팅, 스태깅 등으로 나누어짐 

앙상블 학습은 단일 알고리즘보다 뛰어난 성능을 낸다 
현실은 예측할수없는 다양한 변수와 규칙으로 이루어져있어서 다양한 관점이 더 나은 성능을 낸다



보팅 
- 여러개의 분류기가 투표를 통해 최종 예측결과를 결정하는 방식 ( 서로다른 알고리즘을 결합) 
-  하드보팅 소프트보팅 두가지가 있다. 
- 하드보팅은 다수결의 원칙 
- 소프트보팅은 합산평균을 구해서 결정 

배깅
- 여러개의 분류기가 투표를 통해 최종 예측결과를 결정하는 방식( 서로 같은 알고리즘) 
- 대표적인 알고리즘은 랜덤포레스트 
- 결정트리 기반의 알고리즘으로 각자 결정트리분류기가 학습한뒤 보팅해서 예측 결정 
- 데이터 세트가 중첩되게 분리하는 것을 부트 스트래핑 분할 이라고 한다. 
- 배깅 -> 부트스트랩 어그리게이팅 
- 단점은 하이퍼 파라미터가 너무 많고 튜닝시간이 많이 소요됨 
- 튜닝후 예측성능의 변화가 미미함 

부스팅
- 부스팅은 약한 학습기를 순차적으로 학습 예측하면서 잘못 예측한 데이터에 가중치를 부여해 오류를 개선해 나가는 방식
- 대표적으로는 에이다 부스트 , 그래디언트 부스트 
- 일반적으로 GBM 이 랜덤 포레스트 보다 예측성능이 뛰어남 - 수행시간이 오래걸리고 하이퍼 파라미터 튜닝도 노력이 더필요함 
수행시간은 병렬처리가 되지않기때문에 오래걸림 


XGboost 
- 트리기반 앙상블 학습에서 가장 각광받고 있는 알고리즘 중 하나 
- 캐글에서 상위 과학자가 사용하면서 각광받음
- GBM 기반으로 느린수행시간 및 과적합 문제를 해결함 
- 병렬학습이 가능하다. (GBM 방식에 비해빠름 그러나 다른방식에 비해 빠른건아님) 


LightGBM 
- 부스팅계열에서 각광받는 알고리즘 
- 학습시간이 XGboost 대비 매우 적다 
- 성능은 떨어지지 않으면서 기능적으로 다양성이 좀더 많음
- 적은 데이터셋에 적용시 오버피팅문제가 발생할 여지가 큼
- 1만건이햐 
- 트리분할이 아닌 리프중심 트리분할방식 

스태킹 앙상블 
- 여러 알고리즘을 결합해서 예측결과를 도출 
- 개별알고리즘으로 도출된 결과로 다시 예측을 수행 
- 현실에선 적용 거의안함, 캐글등 성능을 조금이라도 높여야하는 경우 사용 
